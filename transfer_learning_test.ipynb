{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Teste para a implementação de uma rede neural convolucional que utiliza o processo de transfer learning para a classificação de pinturas.\n",
    "\n",
    "Tutoriais seguidos:\n",
    "- https://domino.ai/blog/transfer-learning-in-python#body__048c711080e1\n",
    "- https://neptune.ai/blog/transfer-learning-guide-examples-for-images-and-text-in-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizarei o dataset WikiArt, disponível nesse [link](https://www.kaggle.com/datasets/steubk/wikiart)\n",
    "\n",
    "Para começar, é necessário separar o dataset em três partes:\n",
    "\n",
    "- Treinamento: Utilizada para retreinar as camadas específicas do modelo\n",
    "- Validação: Utilizada para ver se o treinamento do modelo está resultando em overfitting\n",
    "- Teste: Avaliar a acurácia do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(source_dir, train_dir, val_dir, test_dir, split_size=0.8):\n",
    "    \"\"\"\n",
    "    Splits the data from source_dir into train_dir and val_dir based on split_size.\n",
    "    \"\"\"\n",
    "    class_names = os.listdir(source_dir)\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(source_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "            \n",
    "        file_names = os.listdir(class_dir)\n",
    "        file_paths = [os.path.join(class_dir, fname) for fname in file_names]\n",
    "        \n",
    "        train_paths_all, test_paths = train_test_split(file_paths, train_size=split_size, shuffle=True)\n",
    "        \n",
    "        train_paths, val_paths = train_test_split(train_paths_all, train_size=split_size, shuffle=True)\n",
    "        \n",
    "        os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "        \n",
    "        for path in train_paths:\n",
    "            shutil.copy(path, os.path.join(train_dir, class_name))\n",
    "            \n",
    "        for path in val_paths:\n",
    "            shutil.copy(path, os.path.join(val_dir, class_name))\n",
    "        \n",
    "        for path in test_paths:\n",
    "            shutil.copy(path, os.path.join(test_dir, class_name))\n",
    "\n",
    "source_dir = \"C:/Users/jalfr/OneDrive/Desktop/MASP/Code/Test/data/WikiArt/archive\"\n",
    "train_dir = \"C:/Users/jalfr/OneDrive/Desktop/MASP/Code/Test/data/WikiArt/train\"\n",
    "val_dir = \"C:/Users/jalfr/OneDrive/Desktop/MASP/Code/Test/data/WikiArt/val\"\n",
    "test_dir = \"C:/Users/jalfr/OneDrive/Desktop/MASP/Code/Test/data/WikiArt/test\"\n",
    "\n",
    "split_data(source_dir, train_dir, val_dir, test_dir, split_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo - ResNet50\n",
    "\n",
    "Import e utilização do modelo ResNet50, que será a base do projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m102967424/102967424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 0us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6us/step \n",
      "Predicted: [('n02504458', 'African_elephant', 0.83670104), ('n01871265', 'tusker', 0.11563338), ('n02504013', 'Indian_elephant', 0.047628045)]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "model = ResNet50(weights='imagenet')\n",
    "img_path = 'img/elephant.webp'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
